# Unilm结构解析

unilm的预训练过程与bert模型有很大的区别，

|                  | ELMo | GPT  | BERT | Unilm |
| ---------------- | ---- | ---- | ---- | ----- |
| Left-to-Right LM | 是   | 是   |      | 是    |
| Right-to-Left LM | 是   |      | 是   | 是    |
| Bidirectional LM |      |      |      | 是    |
| Seq-to-Seq LM    |      |      |      | 是    |

接下来分析Unilm的微调过程与BERT不一样的操作过程，

**Unilm的微调和BERT不同的区别在于Unilm根据不同的任务加入了不同的掩码模式，从而能够实现预测和seq2seq的不同的任务。**

## 需要使用到的预训练的数据集

unilm对应的预训练文件内容

链接: https://pan.baidu.com/s/1cc9InwUvxfAr3btX4k81VQ  密码: ggo2

小学数学习题对应数据

链接: https://pan.baidu.com/s/1leOy8ldpjpJI3ehB_6uepQ  密码: rglm

## 实现预测时候的掩码操作

使用预测任务有点类似于GPT模型的操作，对应的掩码操作如下：

```python
def lefttoright_compute_attention_bias(self,s):
    seq_len = K.shape(s)[1]
    idxs = K.arange(0, seq_len)
    mask = idxs[None, :] <= idxs[:, None]
    mask = K.cast(mask, K.floatx())
    return -(1-mask) * 1e12
```

对应的矩阵为左下角矩阵，比如输入的矩阵为一个3*3的矩阵(这里不管是什么样的3乘3的矩阵，掩码之后输出的矩阵都是一样的)

进行左下角掩码之后的内容为

```
[[-0.e+00 -1.e+12 -1.e+12]
 [-0.e+00 -0.e+00 -1.e+12]
 [-0.e+00 -0.e+00 -0.e+00]]
```

可以看出，右上方的内容正好被遮盖了起来，能够更好地进行预测操作。

## 实现seq2seq时候的掩码操作

```python
def seq2seq_compute_attention_bias(self,s):
    idxs = K.cumsum(s, axis=1)
    mask =idxs[:, None, :] <= idxs[:, :, None]
    mask = K.cast(mask, K.floatx())
    return -(1-mask) * 1e12
```

这里面传入函数lefttoright_compute_attention_bias的segment_ids的矩阵必须为segment_ids的矩阵，

这里使用segment_ids进行遮盖的原理暂时没有想明白

|      | 0    | 0    | 0    | 1    | 1    | 1    | 0    | 0    | 0    |
| :--: | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|  0   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
|  0   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
|  0   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
|  1   | 白   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   |
|  1   | 白   | 白   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   |
|  1   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
|  0   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
|  0   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
|  0   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |

叠加之后形成的idxs[:,None,:]和idxs[:,:,None]的对应内容为

|      | 0    | 0    | 0    | 1    | 2    | 3    | 3    | 3    | 3    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0    | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
| 0    | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
| 0    | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   | 盖   |
| 1    | 白   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   | 盖   |
| 2    | 白   | 白   | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖   |
| 3    | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
| 3    | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
| 3    | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |
| 3    | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   | 白   |

前面的0为目标，中间的1相当于答案，最后面的0相当于填充部分的内容

这里完整的部分矩阵内容应该是：

|      | 0(0) | 0(0) | 0(0) | 1(1) | 2(1) | 3(1) | 3(0)   | 3(0)   | 3(0)   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------ | ------ | ------ |
| 0(0) | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖     | 盖     | 盖     |
| 0(0) | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖     | 盖     | 盖     |
| 0(0) | 白   | 白   | 白   | 盖   | 盖   | 盖   | 盖     | 盖     | 盖     |
| 1(1) | 白   | 白   | 白   | 白   | 盖   | 盖   | 盖     | 盖     | 盖     |
| 2(1) | 白   | 白   | 白   | 白   | 白   | 盖   | 盖     | 盖     | 盖     |
| 3(1) | 白   | 白   | 白   | 白   | 白   | 白   | 白(盖) | 白(盖) | 白(盖) |
| 3(0) | 白   | 白   | 白   | 白   | 白   | 白   | 白     | 白     | 白     |
| 3(0) | 白   | 白   | 白   | 白   | 白   | 白   | 白     | 白     | 白     |
| 3(0) | 白   | 白   | 白   | 白   | 白   | 白   | 白     | 白     | 白     |

但是由于最后一行的1的内容不需要被盖住，所以形成的矩阵就成为了上面的内容

**这里分析一下最后的一行对应的1的内容**

首先自身不能够预测自身，所以行数与列数相等的地方(比如(1,1),(2,2),(3,3),(4,4))不需要被盖住，显示的内容为白，

**也就是说当需要被预测的情况下**

1.如果不是最后一个1的情况下，将行中后面的1的内容全部盖住(变成盖)

2.如果是最后一个1的情况下，该行全不需要被遮盖(变成白)

因为此时如果是最后一个1的情况下，当前行的这个1因为是自身不需要被遮盖，后续的0也不需要被遮盖，都为对应的白

****

对比下面一种情况

|      | 0    | 0    | 0    | 1    | 1            | 1            |
| ---- | ---- | ---- | ---- | ---- | ------------ | ------------ |
| 0    | 白   | 白   | 白   | 盖   | 盖           | 盖           |
| 0    | 白   | 白   | 白   | 盖   | 盖           | 盖           |
| 0    | 白   | 白   | 白   | 盖   | 盖           | 盖           |
| 1    | 白   | 白   | 白   | 白   | 盖(形成阶梯) | 盖(形成阶梯) |
| 1    | 白   | 白   | 白   | 白   | 白           | 盖(形成阶梯) |
| 1    | 白   | 白   | 白   | 白   | 白           | 白           |

从第四行开始形成阶梯状的矩阵

## **使用seq2seq训练数学题目的过程**

输入过程：数学题id(对应segment_id全0)+答案(对应segment_id全1)+填充部分(对应segment_id全0)

输出部分：本身模型的输出部分为(5,128,1024)，经过了(1024,13584)(13584为字典的总的单词个数)dense层拼接之后，得到的输出部分为(5,128,13584)，接着使用(5,128)与(5,128,13584)计算相应的损失函数，这里使用K.sparse_categorical_crossentropy(y_true,y_pred)来提取出来(5,128)与(5,128,13584)之间的sparse_categorical_crossentropy的损失内容，

**这里能够将输入的id和输出的id合在一起进行训练的原因在于在unilm的过程之中使用了mask掩码，从而保证了只对输出的id内容进行预测。**

比如这个(5,128)之中的一个批次的编码id为(0,2,...102,13583),此时对应的损失函数sparse_categorical_crossentropy的内容为-log(0位置的向量)-log(2位置的向量)-...-log(102位置的向量)-log(13583位置的向量)

**这里使用交叉熵函数的原理**

如果log函数值越小，说明发生的可能性越小，一旦发生了则损失就越大，所以前面需要加上负号。

接下来在使用模型求导的过程中，不断地减少交叉熵损失函数，从而能够提升由输入的id去预测输出的公式的权重矩阵内容。

## 使用seq2seq预测数学公式的过程

输入内容：数学题(对应segment_id全0)

输出部分：对应的数学公式

在训练数学题目的过程中，通过训练模型已经能够很好地使用输入的数学题目去预测对应的数学公式，

**此时使用seq2seq预测数学公式的过程就是一个生成的过程。**

输入(1,18)(注意这里的长度之中没有填充0)，然后经过模型之后输出(1,18,13583)，这里的13583代表词表之中单词的概率，使用beam_search进行下一个单词的预测，每一次预测出来的output_ids都加在之前的token_ids和segment_ids之中，然后继续预测下一位的内容，直到预测的序列达到了最大的长度或者结束的标志达到了指定的个数的时候，终止循环跳出内容，然后将形成的数学公式和对应的答案进行相应的比较。

**注意这里生成的过程需要在生成的概率矩阵之后进行求log并且加上1e-12的操作**

这里的这项操作是为了与之前训练时候的损失函数相对应起来，之前训练的时候使用交叉熵损失函数

```python
output_scores = np.log(output_scores+1e-12)
scores = output_scores+scores
```

小学数学题目解答.py为先训练，所有数据都训练完成之后再进行预测的过程。

bert求解小学数学题目.py为经过训练，并且每一个迭代的epoch都训练完成之后都进行预测的过程。

